{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apify-client\n",
      "  Using cached apify_client-1.8.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting apify-shared>=1.1.2 (from apify-client)\n",
      "  Using cached apify_shared-1.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\sshiv\\appdata\\local\\anaconda3\\lib\\site-packages (from apify-client) (0.27.0)\n",
      "Requirement already satisfied: more_itertools>=10.0.0 in c:\\users\\sshiv\\appdata\\local\\anaconda3\\lib\\site-packages (from apify-client) (10.1.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sshiv\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sshiv\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sshiv\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\sshiv\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sshiv\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sshiv\\appdata\\local\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->apify-client) (0.14.0)\n",
      "Using cached apify_client-1.8.1-py3-none-any.whl (73 kB)\n",
      "Using cached apify_shared-1.2.1-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: apify-shared, apify-client\n",
      "Successfully installed apify-client-1.8.1 apify-shared-1.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install apify-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported 21 jobs to C://Users//sshiv//Documents//GitHub//Job_Trend_Analysis//datasets\\job_listings_data_analyst.csv\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Initialize the Apify client with your API token\n",
    "client = ApifyClient(\"apify_api_GIkHvh2WbTaYlOHBLD7xdL9T9exTSX3W6BiO\")\n",
    "\n",
    "# Run the task and wait for it to finish\n",
    "run = client.task(\"D8t3RVBe3kK5URzpD\").call()\n",
    "\n",
    "# Prepare CSV file and process data\n",
    "output_directory = r'C://Users//sshiv//Documents//GitHub//Job_Trend_Analysis//datasets'\n",
    "csv_filename = os.path.join(output_directory, 'job_listings_data_analyst.csv')\n",
    "fieldnames = set()\n",
    "processed_items = []\n",
    "\n",
    "# Process each item in the dataset\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    # Convert list values to strings\n",
    "    processed_item = {}\n",
    "    for key, value in item.items():\n",
    "        if isinstance(value, list):\n",
    "            processed_item[key] = '; '.join(map(str, value))\n",
    "        else:\n",
    "            processed_item[key] = value\n",
    "    processed_items.append(processed_item)\n",
    "    fieldnames.update(processed_item.keys())\n",
    "\n",
    "# Sort fieldnames for consistent column order\n",
    "sorted_fieldnames = sorted(fieldnames)\n",
    "\n",
    "# Write to CSV file\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=sorted_fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(processed_items)\n",
    "\n",
    "print(f'Successfully exported {len(processed_items)} jobs to {csv_filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported 149 jobs to C://Users//sshiv//Documents//GitHub//Job_Trend_Analysis//datasets\\job_listings_data_scientist.csv\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Initialize the Apify client with your API token\n",
    "client = ApifyClient(\"apify_api_GIkHvh2WbTaYlOHBLD7xdL9T9exTSX3W6BiO\")\n",
    "\n",
    "# Run the task and wait for it to finish\n",
    "run = client.task(\"D8t3RVBe3kK5URzpD\").call()\n",
    "\n",
    "# Prepare CSV file and process data\n",
    "output_directory = r'C://Users//sshiv//Documents//GitHub//Job_Trend_Analysis//datasets'\n",
    "csv_filename = os.path.join(output_directory, 'job_listings_data_scientist.csv')\n",
    "fieldnames = set()\n",
    "processed_items = []\n",
    "\n",
    "# Process each item in the dataset\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    # Convert list values to strings\n",
    "    processed_item = {}\n",
    "    for key, value in item.items():\n",
    "        if isinstance(value, list):\n",
    "            processed_item[key] = '; '.join(map(str, value))\n",
    "        else:\n",
    "            processed_item[key] = value\n",
    "    processed_items.append(processed_item)\n",
    "    fieldnames.update(processed_item.keys())\n",
    "\n",
    "# Sort fieldnames for consistent column order\n",
    "sorted_fieldnames = sorted(fieldnames)\n",
    "\n",
    "# Write to CSV file\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=sorted_fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(processed_items)\n",
    "\n",
    "print(f'Successfully exported {len(processed_items)} jobs to {csv_filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported 142 jobs to C://Users//sshiv//Documents//GitHub//Job_Trend_Analysis//datasets\\job_listings_data_engineer.csv\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Initialize the Apify client with your API token\n",
    "client = ApifyClient(\"apify_api_GIkHvh2WbTaYlOHBLD7xdL9T9exTSX3W6BiO\")\n",
    "\n",
    "# Run the task and wait for it to finish\n",
    "run = client.task(\"D8t3RVBe3kK5URzpD\").call()\n",
    "\n",
    "# Prepare CSV file and process data\n",
    "output_directory = r'C://Users//sshiv//Documents//GitHub//Job_Trend_Analysis//datasets'\n",
    "csv_filename = os.path.join(output_directory, 'job_listings_data_engineer.csv')\n",
    "fieldnames = set()\n",
    "processed_items = []\n",
    "\n",
    "# Process each item in the dataset\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    # Convert list values to strings\n",
    "    processed_item = {}\n",
    "    for key, value in item.items():\n",
    "        if isinstance(value, list):\n",
    "            processed_item[key] = '; '.join(map(str, value))\n",
    "        else:\n",
    "            processed_item[key] = value\n",
    "    processed_items.append(processed_item)\n",
    "    fieldnames.update(processed_item.keys())\n",
    "\n",
    "# Sort fieldnames for consistent column order\n",
    "sorted_fieldnames = sorted(fieldnames)\n",
    "\n",
    "# Write to CSV file\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=sorted_fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(processed_items)\n",
    "\n",
    "print(f'Successfully exported {len(processed_items)} jobs to {csv_filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
